{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0GKVlM+K2M3Z1gSkD6Uj0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JJChrzanowski/GlyCulator3_hotfix/blob/main/DayByDay_GlyCulator_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day-by-day GlyCulator API analysis\n",
        "\n",
        "Please follow these steps:\n",
        "\n",
        "1. Enter your original_session_id and api_key in the variables below.\n",
        "2. Run this cell.\n",
        "\n",
        "The code will:\n",
        " - Connect to the GlyCulator API\n",
        " - Retrieve the original analysis details\n",
        " - Perform day-by-day analysis on each file and date range\n",
        " - Show you the final results as a table\n",
        " - Provide a link to download the results as a CSV file"
      ],
      "metadata": {
        "id": "J2koX6NUMj1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Run once to import required functions\n",
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def get_analysis_details(session_id, api_key, base_url=\"https://glyculator.btm.umed.pl\"):\n",
        "    \"\"\"Retrieve details about an existing analysis.\"\"\"\n",
        "    url = f\"{base_url}/api/analyses/{session_id}?api_key={api_key}\"\n",
        "    response = requests.get(url, headers={\"accept\": \"application/json\"})\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def create_analysis(api_key, data, base_url=\"https://glyculator.btm.umed.pl\"):\n",
        "    \"\"\"Create a new analysis using the provided JSON data.\"\"\"\n",
        "    url = f\"{base_url}/api/analyses/new?api_key={api_key}\"\n",
        "    headers = {\"Content-Type\": \"application/json\", \"accept\": \"application/json\"}\n",
        "    response = requests.post(url, json=data, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def wait_for_analysis_finish(session_id, api_key, base_url=\"https://glyculator.btm.umed.pl\", timeout=600, interval=10):\n",
        "    \"\"\"Polls the API until the analysis is finished or timeout is reached.\"\"\"\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < timeout:\n",
        "        details = get_analysis_details(session_id, api_key, base_url=base_url)\n",
        "        if details.get(\"finished\"):\n",
        "            return details\n",
        "        time.sleep(interval)\n",
        "    raise TimeoutError(f\"Analysis {session_id} did not finish within {timeout} seconds.\")\n",
        "\n",
        "def generate_daily_ranges(start_date_str, end_date_str):\n",
        "    \"\"\"Generate a list of (date_from, date_to) tuples for each day in the range [start_date, end_date].\"\"\"\n",
        "    start_date = datetime.date.fromisoformat(start_date_str)\n",
        "    end_date = datetime.date.fromisoformat(end_date_str)\n",
        "\n",
        "    # Assuming end_date is inclusive\n",
        "    day_ranges = []\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        day_from = current_date\n",
        "        day_to = current_date + datetime.timedelta(days=1)\n",
        "        day_ranges.append((day_from.isoformat(), day_to.isoformat()))\n",
        "        current_date += datetime.timedelta(days=1)\n",
        "    return day_ranges\n",
        "\n",
        "def flatten_indices(indices):\n",
        "    \"\"\"Flatten the nested indices dict into a single-level dict with period-specific prefixes.\"\"\"\n",
        "    flat = {}\n",
        "    for period in [\"whole\", \"day\", \"night\"]:\n",
        "        if period in indices:\n",
        "            for k, v in indices[period].items():\n",
        "                flat[f\"{period}_{k}\"] = v\n",
        "    return flat\n",
        "\n",
        "def remove_analysis_by_session(session_id, api_key, base_url=\"https://glyculator.btm.umed.pl\"):\n",
        "    \"\"\"\n",
        "    Cleanup function to remove an analysis by session_id.\n",
        "    This function calls the GET endpoint:\n",
        "    /api/remove_analysis_by_session?session_id={session_id}&api_key={api_key}\n",
        "    \"\"\"\n",
        "    url = f\"{base_url}/api/remove_analysis_by_session?session_id={session_id}&api_key={api_key}\"\n",
        "    response = requests.get(url, headers={\"accept\": \"*/*\"})\n",
        "    response.raise_for_status()\n",
        "    print(f'Session {session_id} removed as cleanup.')\n",
        "\n",
        "def compute_day_by_day_indices(original_session_id, api_key, cleanup=False):\n",
        "    # 1. Get the original analysis details\n",
        "    original_details = get_analysis_details(original_session_id, api_key)\n",
        "\n",
        "    # Extract the configuration parameters\n",
        "    imputation_method = original_details.get(\"imputation_method\")\n",
        "    imputation_max_gap = original_details.get(\"imputation_max_gap\")\n",
        "    imputation_padding = original_details.get(\"imputation_padding\")\n",
        "    tbr_low = original_details.get(\"tbr_low\")\n",
        "    tbr_high = original_details.get(\"tbr_high\")\n",
        "    tar_low = original_details.get(\"tar_low\")\n",
        "    tar_high = original_details.get(\"tar_high\")\n",
        "    glucose_unit = original_details.get(\"glucose_unit\")\n",
        "    night_start = original_details.get(\"night_start\")\n",
        "    night_end = original_details.get(\"night_end\")\n",
        "\n",
        "    # The original name can be reused or modified\n",
        "    base_name = original_details.get(\"name\", \"DayByDay_Analysis\")\n",
        "\n",
        "    # 2. Extract the analysis_files from the original\n",
        "    analysis_files = original_details.get(\"analysis_files\", [])\n",
        "\n",
        "    # We'll store results here\n",
        "    day_by_day_results = []\n",
        "\n",
        "    # 3. For each analysis_file, generate daily segments and run new analyses\n",
        "    for file_index, analysis_file in enumerate(analysis_files):\n",
        "        file_hash = analysis_file[\"file\"][\"file_hash\"]\n",
        "        filename = analysis_file[\"file\"][\"filename\"]\n",
        "\n",
        "        date_from = analysis_file[\"date_from\"]\n",
        "        date_to = analysis_file[\"date_to\"]\n",
        "\n",
        "        # Generate daily ranges\n",
        "        daily_ranges = generate_daily_ranges(date_from, date_to)\n",
        "\n",
        "        for i, (day_start, day_end) in enumerate(daily_ranges):\n",
        "            # Build request body for a new analysis\n",
        "            new_analysis_data = {\n",
        "                \"name\": f\"{base_name}_File{file_index+1}_Day{i+1}\",\n",
        "                \"imputation_method\": imputation_method,\n",
        "                \"imputation_max_gap\": imputation_max_gap,\n",
        "                \"imputation_padding\": imputation_padding,\n",
        "                \"tbr_low\": tbr_low,\n",
        "                \"tbr_high\": tbr_high,\n",
        "                \"tar_low\": tar_low,\n",
        "                \"tar_high\": tar_high,\n",
        "                \"glucose_unit\": glucose_unit,\n",
        "                \"night_start\": night_start,\n",
        "                \"night_end\": night_end,\n",
        "                \"analysis_files\": [\n",
        "                    {\n",
        "                        \"date_from\": day_start,\n",
        "                        \"date_to\": day_end,\n",
        "                        \"file\": {\n",
        "                            \"file_hash\": file_hash\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            # 4. Create a new analysis for this single-day segment\n",
        "            try:\n",
        "              creation_response = create_analysis(api_key, new_analysis_data)\n",
        "              new_session_id = creation_response.get(\"session_id\")\n",
        "\n",
        "              if not new_session_id:\n",
        "                  print(\"No session_id returned from new analysis creation. Skipping.\")\n",
        "                  continue\n",
        "\n",
        "              # 5. Wait for the analysis to finish\n",
        "              finished_details = wait_for_analysis_finish(new_session_id, api_key)\n",
        "\n",
        "              # Extract day-by-day indices\n",
        "              if \"analysis_files\" in finished_details and finished_details[\"analysis_files\"]:\n",
        "                  day_indices = finished_details[\"analysis_files\"][0][\"indices\"]\n",
        "\n",
        "                  # Append results with filename and file_hash included\n",
        "                  day_by_day_results.append({\n",
        "                      \"session_id\": new_session_id,\n",
        "                      \"file_hash\": file_hash,\n",
        "                      \"filename\": filename,\n",
        "                      \"date_from\": day_start,\n",
        "                      \"date_to\": day_end,\n",
        "                      \"indices\": day_indices\n",
        "                  })\n",
        "\n",
        "                  # Cleanup a particular session once done if requested\n",
        "                  if cleanup:\n",
        "                      remove_analysis_by_session(new_session_id, api_key)\n",
        "\n",
        "              # Print progress\n",
        "              print(f\"Completed day-by-day analysis for {day_start} to {day_end}, session_id: {new_session_id}\")\n",
        "            except:\n",
        "              print(f\"Error creating new analysis for {day_start} to {day_end} - probably no data for given day. Skipping.\")\n",
        "\n",
        "    # Convert day_by_day_results to a pandas DataFrame\n",
        "    records = []\n",
        "    for res in day_by_day_results:\n",
        "        flat_indices = flatten_indices(res[\"indices\"])\n",
        "        record = {\n",
        "            \"session_id\": res[\"session_id\"],\n",
        "            \"file_hash\": res[\"file_hash\"],\n",
        "            \"filename\": res[\"filename\"],\n",
        "            \"date_from\": res[\"date_from\"],\n",
        "            \"date_to\": res[\"date_to\"]\n",
        "        }\n",
        "        # Merge flattened indices into the record\n",
        "        record.update(flat_indices)\n",
        "        records.append(record)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    return df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uyMtgNuPMsBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NEVER SHARE YOUR API KEY!!!\n",
        "original_session_id = \"YOUR_SESSION_ID\"\n",
        "api_key = \"YOUR_API_KEY\""
      ],
      "metadata": {
        "id": "vFP7VWdpMmGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example use\n",
        "df_results = compute_day_by_day_indices(original_session_id, api_key, cleanup=False)\n",
        "df_results.head()"
      ],
      "metadata": {
        "id": "4DpR5lypM6Ua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}