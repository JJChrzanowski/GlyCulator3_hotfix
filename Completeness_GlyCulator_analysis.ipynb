{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx7kF8TyxP7DDRqcoNOsk3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JJChrzanowski/GlyCulator3_hotfix/blob/main/Completeness_GlyCulator_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Completeness fix for GlyCulator analysis using GlyCulator API\n",
        "\n",
        "Please follow these steps:\n",
        "\n",
        "*   Enter your original_session_id and api_key in the variables below.\n",
        "*   Run this cell.\n",
        "\n",
        "The code will:\n",
        "\n",
        "*   Connect to the GlyCulator API\n",
        "*   Retrieve the original analysis details\n",
        "*   Find appropraite data to compute completeness for your predefined files and periods\n",
        "*   Show you the final results as a table\n",
        "*   Provide a link to download the results as a CSV file\n",
        "\n"
      ],
      "metadata": {
        "id": "rrZADQaSc4eF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E57scLF-VRy4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "base_url = \"https://glyculator.btm.umed.pl/api\"\n",
        "\n",
        "def get_analysis(session_id, api_key):\n",
        "    url = f\"{base_url}/analyses/{session_id}?api_key={api_key}\"\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def get_file_info(file_hash, api_key):\n",
        "    url = f\"{base_url}/files/{file_hash}?api_key={api_key}\"\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def get_analysis_raw_data_csv(session_id, api_key):\n",
        "    url = f\"{base_url}/analysis_raw_data_csv?session_id={session_id}&api_key={api_key}\"\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    # It's a CSV text; convert directly to a DataFrame\n",
        "    from io import StringIO\n",
        "    return pd.read_csv(StringIO(r.text))\n",
        "\n",
        "def compute_completeness_no_imputation(file_info, date_from, date_to):\n",
        "    if not file_info or 'basic_stats' not in file_info:\n",
        "        return None\n",
        "    start_period = pd.to_datetime(date_from)\n",
        "    end_period = pd.to_datetime(date_to) + timedelta(days=1) - timedelta(seconds=1)\n",
        "\n",
        "    mindate = pd.to_datetime(file_info['basic_stats']['mindate'])\n",
        "    maxdate = pd.to_datetime(file_info['basic_stats']['maxdate'])\n",
        "\n",
        "    adjusted_start = max(mindate, start_period)\n",
        "    adjusted_end = min(maxdate, end_period)\n",
        "\n",
        "    if adjusted_start >= adjusted_end:\n",
        "        return None\n",
        "\n",
        "    max_count = file_info['basic_stats']['max_count']\n",
        "    day_data = pd.DataFrame(file_info['basic_stats']['data'])\n",
        "    day_data['date'] = pd.to_datetime(day_data['date']).dt.date\n",
        "\n",
        "    start_date = adjusted_start.date()\n",
        "    end_date = adjusted_end.date()\n",
        "\n",
        "    mask = (day_data['date'] >= start_date) & (day_data['date'] <= end_date)\n",
        "    filtered = day_data[mask]\n",
        "\n",
        "    total_count = filtered['count'].sum()\n",
        "    total_hours = (adjusted_end - adjusted_start).total_seconds() / 3600.0\n",
        "    expected_count = (total_hours / 24.0) * max_count if max_count > 0 else None\n",
        "    if expected_count is None or expected_count == 0:\n",
        "        return None\n",
        "\n",
        "    completeness = total_count / expected_count\n",
        "    return completeness\n",
        "\n",
        "\n",
        "def compute_completeness_with_imputation(raw_data_df, file_hash, date_from, date_to, max_count):\n",
        "    # Filter by file_hash and date range\n",
        "    # raw_data_df columns: Filename, Date, Glucose, Type\n",
        "    mask_file = raw_data_df['Filename'].eq(file_hash)\n",
        "    raw_data_df['Date_dt'] = pd.to_datetime(raw_data_df['Date'])\n",
        "    start = pd.to_datetime(date_from)\n",
        "    end = pd.to_datetime(date_to)\n",
        "    mask_date = (raw_data_df['Date_dt'] >= start) & (raw_data_df['Date_dt'] <= end)\n",
        "    filtered = raw_data_df[mask_file & mask_date]\n",
        "\n",
        "    # Compute num_days\n",
        "    num_days = (end.date() - start.date()).days + 1\n",
        "    total_count = len(filtered)\n",
        "    expected_count = max_count * num_days\n",
        "    completeness = total_count / expected_count if expected_count > 0 else None\n",
        "    return completeness"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEVER SHARE YOUR API KEY!!!\n",
        "API_KEY = \"API_KEY\"\n",
        "SESSION_ID = \"SESSION_ID\""
      ],
      "metadata": {
        "id": "olXCDyB-c6mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main logic\n",
        "analysis = get_analysis(SESSION_ID, API_KEY)\n",
        "imputation_method = analysis.get('imputation_method')\n",
        "\n",
        "fixed_results = []\n",
        "for af in analysis['analysis_files']:\n",
        "    date_from = af['date_from']\n",
        "    date_to = af['date_to']\n",
        "    fhash = af['file']['file_hash']\n",
        "\n",
        "    # If no imputation method: we can directly use file info\n",
        "    if analysis['imputation_method'] is None:\n",
        "        file_info = get_file_info(fhash, API_KEY)\n",
        "        max_count = file_info['basic_stats']['max_count']\n",
        "        completeness = compute_completeness_no_imputation(file_info, date_from, date_to)\n",
        "    else:\n",
        "        # Imputation is not null, fallback to raw_data_csv\n",
        "        # This might be slow!\n",
        "        raw_data = get_analysis_raw_data_csv(SESSION_ID, API_KEY)\n",
        "        # We still need max_count from file info because raw_data doesn't have it\n",
        "        file_info = get_file_info(fhash, API_KEY)\n",
        "        max_count = file_info['basic_stats']['max_count']\n",
        "        completeness = compute_completeness_with_imputation(raw_data, fhash, date_from, date_to, max_count)\n",
        "\n",
        "    fixed_results.append({\n",
        "        'filename': af['file']['filename'],\n",
        "        'file_hash': fhash,\n",
        "        'date_from': date_from,\n",
        "        'date_to': date_to,\n",
        "        'original_completeness': af['indices']['whole']['completeness'] if 'indices' in af and 'whole' in af['indices'] else None,\n",
        "        'fixed_completeness': completeness\n",
        "    })\n",
        "\n",
        "# fixed_results now contains corrected completeness values for each file.\n",
        "# Next step could be to PATCH them back if there's an endpoint or store them locally.\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results\n"
      ],
      "metadata": {
        "id": "-8D3Vp3Dc6zR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}