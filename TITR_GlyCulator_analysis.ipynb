{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMedKSqA+unyNDik9GtWhym",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JJChrzanowski/GlyCulator3_hotfix/blob/main/TITR_GlyCulator_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## TITR rerun analysis with GlyCulator API\n",
        "\n",
        "Please follow these steps:\n",
        "\n",
        "*   Enter your original_session_id and api_key in the variables below.\n",
        "*   Run this cell.\n",
        "\n",
        "The code will:\n",
        "\n",
        "*   Connect to the GlyCulator API\n",
        "*   Retrieve the original analysis details\n",
        "*   Perform analysis with TITR for each file and date range\n",
        "*   Show you the final results as a table\n",
        "*   Provide a link to download the results as a CSV file\n",
        "\n"
      ],
      "metadata": {
        "id": "hfatqs7wpxIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "36PtiDpRprb5"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import requests\n",
        "import time\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "def get_analysis_details(session_id, api_key, base_url=\"https://glyculator.btm.umed.pl\"):\n",
        "    url = f\"{base_url}/api/analyses/{session_id}?api_key={api_key}\"\n",
        "    response = requests.get(url, headers={\"accept\": \"application/json\"})\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def create_analysis_with_retries(api_key, data, base_url=\"https://glyculator.btm.umed.pl\", max_retries=3, wait_seconds=30):\n",
        "    \"\"\"Create a new analysis with retries on 500 Internal Server Error.\"\"\"\n",
        "    url = f\"{base_url}/api/analyses/new?api_key={api_key}\"\n",
        "    headers = {\"Content-Type\": \"application/json\", \"accept\": \"application/json\"}\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, json=data, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.HTTPError as e:\n",
        "            # Check if it's a 500 error\n",
        "            if response.status_code == 500:\n",
        "                print(f\"Got 500 Internal Server Error (attempt {attempt+1}/{max_retries}). Waiting {wait_seconds} seconds and retrying...\")\n",
        "                time.sleep(wait_seconds)\n",
        "            else:\n",
        "                raise e\n",
        "    raise RuntimeError(\"Failed to create analysis after multiple retries due to 500 errors.\")\n",
        "\n",
        "def wait_for_analysis_finish(session_id, api_key, base_url=\"https://glyculator.btm.umed.pl\", timeout=1200, interval=15):\n",
        "    \"\"\"Wait longer and poll less frequently if analysis takes time.\"\"\"\n",
        "    start_time = time.time()\n",
        "    while time.time() - start_time < timeout:\n",
        "        details = get_analysis_details(session_id, api_key, base_url=base_url)\n",
        "        if details.get(\"finished\"):\n",
        "            return details\n",
        "        time.sleep(interval)\n",
        "    raise TimeoutError(f\"Analysis {session_id} did not finish within {timeout} seconds.\")\n",
        "\n",
        "def flatten_indices(indices):\n",
        "    flat = {}\n",
        "    for period in [\"whole\", \"day\", \"night\"]:\n",
        "        if period in indices:\n",
        "            for k, v in indices[period].items():\n",
        "                flat[f\"{period}_{k}\"] = v\n",
        "    return flat\n",
        "\n",
        "def compute_titr_indices(original_session_id, api_key):\n",
        "    # Get the original analysis details\n",
        "    original_details = get_analysis_details(original_session_id, api_key)\n",
        "\n",
        "    # Extract configuration parameters\n",
        "    imputation_method = original_details.get(\"imputation_method\")\n",
        "    imputation_max_gap = original_details.get(\"imputation_max_gap\")\n",
        "    imputation_padding = original_details.get(\"imputation_padding\")\n",
        "    tbr_low = original_details.get(\"tbr_low\")\n",
        "    tar_high = original_details.get(\"tar_high\")\n",
        "    glucose_unit = original_details.get(\"glucose_unit\")\n",
        "    night_start = original_details.get(\"night_start\")\n",
        "    night_end = original_details.get(\"night_end\")\n",
        "\n",
        "    original_name = original_details.get(\"name\", \"Original_Analysis\")\n",
        "    original_analysis_files = original_details.get(\"analysis_files\", [])\n",
        "\n",
        "    # Map original data\n",
        "    original_data_map = {}\n",
        "    for f in original_analysis_files:\n",
        "        file_hash = f[\"file\"][\"file_hash\"]\n",
        "        filename = f[\"file\"][\"filename\"]\n",
        "        date_from = f[\"date_from\"]\n",
        "        date_to = f[\"date_to\"]\n",
        "        original_indices = f[\"indices\"]\n",
        "        original_flat = flatten_indices(original_indices)\n",
        "\n",
        "        original_data_map[(file_hash, date_from, date_to)] = {\n",
        "            \"filename\": filename,\n",
        "            \"file_hash\": file_hash,\n",
        "            \"date_from\": date_from,\n",
        "            \"date_to\": date_to,\n",
        "            **original_flat\n",
        "        }\n",
        "\n",
        "    # Prepare new analysis with modified tbr_high and tar_low for all files at once\n",
        "    new_analysis_data = {\n",
        "        \"name\": f\"{original_name}_TightRange\",\n",
        "        \"imputation_method\": imputation_method,\n",
        "        \"imputation_max_gap\": imputation_max_gap,\n",
        "        \"imputation_padding\": imputation_padding,\n",
        "        \"tbr_low\": tbr_low,\n",
        "        \"tbr_high\": 70,\n",
        "        \"tar_low\": 140,\n",
        "        \"tar_high\": tar_high,\n",
        "        \"glucose_unit\": glucose_unit,\n",
        "        \"night_start\": night_start,\n",
        "        \"night_end\": night_end,\n",
        "        \"analysis_files\": []\n",
        "    }\n",
        "\n",
        "    # Add all files\n",
        "    for f in original_analysis_files:\n",
        "        file_hash = f[\"file\"][\"file_hash\"]\n",
        "        date_from = f[\"date_from\"]\n",
        "        date_to = f[\"date_to\"]\n",
        "        new_analysis_data[\"analysis_files\"].append({\n",
        "            \"date_from\": date_from,\n",
        "            \"date_to\": date_to,\n",
        "            \"file\": {\n",
        "                \"file_hash\": file_hash\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Create new analysis with retries on 500 error\n",
        "    creation_response = create_analysis_with_retries(api_key, new_analysis_data)\n",
        "    new_session_id = creation_response.get(\"session_id\")\n",
        "    if not new_session_id:\n",
        "        raise RuntimeError(\"No session_id returned from new tight-range analysis creation.\")\n",
        "\n",
        "    # Wait for the new analysis to finish (extended timeout and interval)\n",
        "    finished_details = wait_for_analysis_finish(new_session_id, api_key)\n",
        "\n",
        "    new_analysis_files = finished_details.get(\"analysis_files\", [])\n",
        "\n",
        "    records = []\n",
        "    for new_file in new_analysis_files:\n",
        "        file_hash = new_file[\"file\"][\"file_hash\"]\n",
        "        filename = new_file[\"file\"][\"filename\"]\n",
        "        date_from = new_file[\"date_from\"]\n",
        "        date_to = new_file[\"date_to\"]\n",
        "        tight_indices = new_file[\"indices\"]\n",
        "        tight_flat = flatten_indices(tight_indices)\n",
        "\n",
        "        # Extract tight-range tir metrics and rename\n",
        "        whole_titr = tight_flat.get(\"whole_tir\")\n",
        "        day_titr = tight_flat.get(\"day_tir\")\n",
        "        night_titr = tight_flat.get(\"night_tir\")\n",
        "\n",
        "        key = (file_hash, date_from, date_to)\n",
        "        if key not in original_data_map:\n",
        "            continue\n",
        "\n",
        "        original_record = original_data_map[key].copy()\n",
        "        original_record[\"whole_titr\"] = whole_titr\n",
        "        original_record[\"day_titr\"] = day_titr\n",
        "        original_record[\"night_titr\"] = night_titr\n",
        "\n",
        "        records.append(original_record)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEVER SHARE YOUR API KEY!!!\n",
        "original_session_id = \"YOUR_SESSION_ID\"\n",
        "api_key = \"YOUR_API_KEY\""
      ],
      "metadata": {
        "id": "ushQhkOkqKUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example use\n",
        "df_results = compute_titr_indices(original_session_id, api_key, cleanup=False)\n",
        "df_results.head()"
      ],
      "metadata": {
        "id": "EV31IfCqqNU0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}